# Fooling a neural network with common adversarial noise #


**Abstract**
<p align="justify">
These days deep Neural Networks (NN) show exceptional performance on speech and visual recognition tasks. These systems are still considered a black box without deep understanding why they perform in such a manner. This lack of understanding makes NNs vulnerable to specially crafted adversarial examples - inputs with small perturbations that make the model misclassify. In this paper, we generated adversarial examples that will fool a NN used for classifying handwritten digits. We start by generating additive adversarial noise for each image, then we craft a single adversarial noise that misclassifies different members of the same class.
</p>

The full research is available at [Common adversarial noise for fooling a neural network](https://github.com/Maki94/cnn_adv_examples/blob/master/demos/Fooling%20a%20neural%20network%20with%20common%20adversarial%20noise.pdf)
