# Common adversarial noise for fooling a neural network

**Abstract**

These days deep Neural Networks (NN) show exceptional performance on speech and visual recognition tasks. These systems are still considered a black box without deep understanding why they perform in such a manner. This lack of understanding makes NNs vulnerable to specially crafted adversarial examples - inputs with small perturbations that make the model misclassify. In this paper, we generated adversarial examples that will fool a NN used for classifying handwritten digits. We start by generating additive adversarial noise for each image, then we craft a single adversarial noise that misclassifies different members of the same class.

Full paper is available at [Common adversarial noise for fooling a neural network](https://github.com/Maki94/cnn_adv_examples/blob/master/demos/Common%20adversarial%20noise%20for%20fooling%20a%20neural%20network.pdf)
